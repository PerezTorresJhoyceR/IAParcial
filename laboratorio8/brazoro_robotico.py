# -*- coding: utf-8 -*-
"""brazoRobotico.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P5dUgWfv_J6XbE0MKUZcIpe_LA1toPWa
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import MultivariateNormal
import matplotlib.pyplot as plt
from collections import deque
import random
import math
from typing import Tuple, List, Dict, Optional
import cv2

class RoboticArmEnvironment:
    """
    Entorno simulado del brazo robótico para manipulación de objetos
    """
    def __init__(self, workspace_size: float = 1.0, max_steps: int = 200):
        self.workspace_size = workspace_size
        self.max_steps = max_steps

        # Configuración del brazo robótico (3 DOF)
        self.num_joints = 3
        self.link_lengths = np.array([0.3, 0.25, 0.2])  # Longitudes de los eslabones
        self.joint_limits = np.array([[-np.pi, np.pi]] * self.num_joints)  # Límites articulares

        # Estado del sistema
        self.joint_angles = np.zeros(self.num_joints)
        self.joint_velocities = np.zeros(self.num_joints)
        self.cup_position = np.zeros(2)  # Posición del vaso en 2D
        self.cup_upright = True  # Estado del vaso (vertical/volteado)

        # Variables de episodio
        self.step_count = 0
        self.episode_reward = 0
        self.target_reached = False

        # Parámetros de recompensa
        self.flip_reward = 100.0
        self.approach_reward_scale = 10.0
        self.collision_penalty = -50.0
        self.time_penalty = -0.1
        self.out_of_bounds_penalty = -20.0

        # Configuración visual
        self.image_size = (64, 64)
        self.reset()

    def reset(self) -> np.ndarray:
        """Reinicia el entorno para un nuevo episodio"""
        # Posición inicial del brazo
        self.joint_angles = np.random.uniform(-0.2, 0.2, self.num_joints)
        self.joint_velocities = np.zeros(self.num_joints)

        # Posición aleatoria del vaso dentro del área de trabajo
        angle = np.random.uniform(0, 2 * np.pi)
        distance = np.random.uniform(0.3, 0.8)
        self.cup_position = np.array([
            distance * np.cos(angle),
            distance * np.sin(angle)
        ])

        self.cup_upright = True
        self.step_count = 0
        self.episode_reward = 0
        self.target_reached = False

        return self._get_observation()

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """Ejecuta una acción en el entorno"""
        self.step_count += 1

        # Aplicar acción (cambios en ángulos articulares y activación de empuje)
        joint_actions = action[:self.num_joints]
        push_action = action[self.num_joints]  # Acción de empuje

        # Actualizar ángulos articulares
        self.joint_velocities += joint_actions * 0.1
        self.joint_velocities = np.clip(self.joint_velocities, -1.0, 1.0)
        self.joint_angles += self.joint_velocities * 0.02

        # Aplicar límites articulares
        for i in range(self.num_joints):
            self.joint_angles[i] = np.clip(
                self.joint_angles[i],
                self.joint_limits[i][0],
                self.joint_limits[i][1]
            )

        # Calcular posición del efector final
        end_effector_pos = self._forward_kinematics()

        # Calcular recompensa
        reward = self._calculate_reward(end_effector_pos, push_action)

        # Verificar condiciones de terminación
        done = self._is_done(end_effector_pos)

        # Información adicional
        info = {
            'step': self.step_count,
            'end_effector_pos': end_effector_pos,
            'cup_position': self.cup_position,
            'distance_to_cup': np.linalg.norm(end_effector_pos - self.cup_position),
            'cup_flipped': not self.cup_upright
        }

        self.episode_reward += reward

        return self._get_observation(), reward, done, info

    def _forward_kinematics(self) -> np.ndarray:
        """Calcula la posición del efector final usando cinemática directa"""
        x, y = 0, 0
        cumulative_angle = 0

        for i in range(self.num_joints):
            cumulative_angle += self.joint_angles[i]
            x += self.link_lengths[i] * np.cos(cumulative_angle)
            y += self.link_lengths[i] * np.sin(cumulative_angle)

        return np.array([x, y])

    def _get_observation(self) -> np.ndarray:
        """Obtiene la observación actual del estado"""
        # Estado del brazo
        arm_state = np.concatenate([
            self.joint_angles,
            self.joint_velocities,
            self._forward_kinematics()
        ])

        # Estado del vaso
        cup_state = np.concatenate([
            self.cup_position,
            [float(self.cup_upright)]
        ])

        # Información visual simulada (características extraídas)
        visual_features = self._get_visual_features()

        return self._normalize(np.concatenate([arm_state, cup_state, visual_features]))

    def _get_visual_features(self) -> np.ndarray:
        """Simula características visuales extraídas de la cámara"""
        end_effector_pos = self._forward_kinematics()

        # Características relacionales
        distance_to_cup = np.linalg.norm(end_effector_pos - self.cup_position)
        angle_to_cup = np.arctan2(
            self.cup_position[1] - end_effector_pos[1],
            self.cup_position[0] - end_effector_pos[0]
        )

        # Características normalizadas del espacio de trabajo
        normalized_cup_pos = self.cup_position / self.workspace_size
        normalized_ee_pos = end_effector_pos / self.workspace_size

        return np.array([
            distance_to_cup,
            angle_to_cup,
            normalized_cup_pos[0],
            normalized_cup_pos[1],
            normalized_ee_pos[0],
            normalized_ee_pos[1]
        ])

    def _calculate_reward(self, end_effector_pos: np.ndarray, push_action: float) -> float:
        """Calcula la recompensa basada en el estado actual"""
        reward = 0.0

        # Distancia al vaso
        distance_to_cup = np.linalg.norm(end_effector_pos - self.cup_position)

        # Recompensa por acercarse al vaso
        max_distance = np.sqrt(2) * self.workspace_size
        approach_reward = self.approach_reward_scale * (1 - distance_to_cup / max_distance)
        reward += approach_reward

        # Recompensa por voltear el vaso
        if distance_to_cup < 0.05 and push_action > 0.5 and self.cup_upright:
            self.cup_upright = False
            self.target_reached = True
            reward += self.flip_reward

        # Penalización por salirse del área de trabajo
        if np.linalg.norm(end_effector_pos) > self.workspace_size:
            reward += self.out_of_bounds_penalty

        # Penalización por tiempo
        reward += self.time_penalty

        return reward

    def _is_done(self, end_effector_pos: np.ndarray) -> bool:
        """Determina si el episodio ha terminado"""
        # Episodio exitoso
        if self.target_reached:
            return True

        # Límite de tiempo
        if self.step_count >= self.max_steps:
            return True

        # Fuera del área de trabajo
        if np.linalg.norm(end_effector_pos) > self.workspace_size * 1.2:
            return True

        return False

    def render(self):
        """Visualiza el estado actual del entorno"""
        plt.figure(figsize=(8, 8))
        plt.xlim(-self.workspace_size, self.workspace_size)
        plt.ylim(-self.workspace_size, self.workspace_size)

        # Dibujar brazo robótico
        positions = [(0, 0)]
        cumulative_angle = 0
        x, y = 0, 0

        for i in range(self. num_joints):
            cumulative_angle += self.joint_angles[i]
            x += self.link_lengths[i] * np.cos(cumulative_angle)
            y += self.link_lengths[i] * np.sin(cumulative_angle)
            positions.append((x, y))

        # Dibujar eslabones
        for i in range(len(positions) - 1):
            plt.plot([positions[i][0], positions[i+1][0]],
                    [positions[i][1], positions[i+1][1]], 'b-', linewidth=3)

        # Dibujar articulaciones
        plt.annotate("Efector", (positions[-1][0], positions[-1][1]), textcoords="offset points", xytext=(0,10), ha='center')
        for pos in positions:
            plt.plot(pos[0], pos[1], 'ro', markersize=8)

        # Dibujar vaso
        plt.annotate("Vaso", (self.cup_position[0], self.cup_position[1]), textcoords="offset points", xytext=(0,10), ha='center')
        cup_color = 'green' if self.cup_upright else 'red'
        plt.plot(self.cup_position[0], self.cup_position[1], 'o',
                color=cup_color, markersize=15, label=f'Vaso ({"Vertical" if self.cup_upright else "Volteado"})')

        # Dibujar área de trabajo
        circle = plt.Circle((0, 0), self.workspace_size, fill=False, linestyle='--', alpha=0.5)
        plt.gca().add_patch(circle)

        plt.title(f'Brazo Robótico - Paso {self.step_count}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axis('equal')
        plt.pause(0.01)
        plt.clf()


class PolicyNetwork(nn.Module):
    """Red neuronal para la política del agente PPO"""
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(PolicyNetwork, self).__init__()

        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()  # Acciones en rango [-1, 1]
        )

        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # Parámetros de la distribución de acciones
        self.action_var = nn.Parameter(torch.full((action_dim,), 0.5))

    def forward(self, state):
        action_mean = self.actor(state)
        return action_mean

    def act(self, state):
        action_mean = self.actor(state)
        cov_mat = torch.diag(self.action_var).unsqueeze(0)
        dist = MultivariateNormal(action_mean, cov_mat)

        action = dist.sample()
        action_logprob = dist.log_prob(action)
        state_value = self.critic(state)

        return action.detach(), action_logprob.detach(), state_value.detach()

    def evaluate(self, state, action):
        action_mean = self.actor(state)
        cov_mat = torch.diag(self.action_var).expand_as(action_mean)
        cov_mat = torch.diag_embed(cov_mat)

        dist = MultivariateNormal(action_mean, cov_mat)
        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_values = self.critic(state)

        return action_logprobs, state_values, dist_entropy


class PPOAgent:
    """Agente PPO (Proximal Policy Optimization)"""
    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4,
                 eps_clip: float = 0.2, k_epochs: int = 4, gamma: float = 0.99):
        self.lr = lr
        self.eps_clip = eps_clip
        self.k_epochs = k_epochs
        self.gamma = gamma

        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.policy_old = PolicyNetwork(state_dim, action_dim)
        self.policy_old.load_state_dict(self.policy.state_dict())

        self.MseLoss = nn.MSELoss()

        # Buffer de memoria
        self.buffer = {
            'states': [],
            'actions': [],
            'logprobs': [],
            'rewards': [],
            'state_values': [],
            'is_terminals': []
        }

    def select_action(self, state):
        """Selecciona una acción basada en la política actual"""
        with torch.no_grad():
            state = torch.FloatTensor(state).unsqueeze(0)
            action, action_logprob, state_value = self.policy_old.act(state)

        self.buffer['states'].append(state.squeeze(0))
        self.buffer['actions'].append(action.squeeze(0))
        self.buffer['logprobs'].append(action_logprob.squeeze(0))
        self.buffer['state_values'].append(state_value.squeeze(0))

        return action.squeeze(0).numpy()

    def store_reward(self, reward, is_terminal):
        """Almacena recompensa en el buffer"""
        self.buffer['rewards'].append(reward)
        self.buffer['is_terminals'].append(is_terminal)

    def update(self):
        """Actualiza la política usando PPO"""
        # Calcular recompensas descontadas
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(self.buffer['rewards']),
                                     reversed(self.buffer['is_terminals'])):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)

        # Normalizar recompensas
        rewards = torch.tensor(rewards, dtype=torch.float32)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        # Convertir listas a tensores
        old_states = torch.stack(self.buffer['states']).detach()
        old_actions = torch.stack(self.buffer['actions']).detach()
        old_logprobs = torch.stack(self.buffer['logprobs']).detach()
        old_state_values = torch.stack(self.buffer['state_values']).detach()

        # Calcular ventajas
        advantages = rewards - old_state_values

        # Optimización PPO
        for _ in range(self.k_epochs):
            # Evaluar acciones actuales
            logprobs, state_values, dist_entropy = self.policy.evaluate(
                old_states, old_actions)

            # Calcular ratio
            ratios = torch.exp(logprobs - old_logprobs)

            # Calcular pérdida surrogate
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages

            # Pérdida total
            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(
                state_values.squeeze(), rewards) - 0.01 * dist_entropy

            # Actualizar parámetros
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

        # Copiar política nueva a política antigua
        self.policy_old.load_state_dict(self.policy.state_dict())

        # Limpiar buffer
        self.clear_buffer()

    def clear_buffer(self):
        """Limpia el buffer de memoria"""
        self.buffer = {
            'states': [],
            'actions': [],
            'logprobs': [],
            'rewards': [],
            'state_values': [],
            'is_terminals': []
        }

    def save_model(self, filepath: str):
        """Guarda el modelo entrenado"""
        torch.save(self.policy.state_dict(), filepath)

    def load_model(self, filepath: str):
        """Carga un modelo pre-entrenado"""
        self.policy.load_state_dict(torch.load(filepath))
        self.policy_old.load_state_dict(torch.load(filepath))


class TrainingManager:
    """Gestor del entrenamiento del agente"""
    def __init__(self, env: RoboticArmEnvironment, agent: PPOAgent):
        self.env = env
        self.agent = agent
        self.training_stats = {
            'episodes': [],
            'rewards': [],
            'success_rate': [],
            'average_steps': []
        }

    def train(self, max_episodes: int = 1000, update_interval: int = 20,
              print_interval: int = 50):
        """Entrena el agente"""
        print("Iniciando entrenamiento del brazo robótico...")
        print(f"Episodios máximos: {max_episodes}")
        print(f"Intervalo de actualización: {update_interval}")
        print("-" * 50)

        episode_rewards = []
        success_count = 0

        for episode in range(1, max_episodes + 1):
            state = self.env.reset()
            episode_reward = 0

            for step in range(self.env.max_steps):
                # Seleccionar acción
                action = self.agent.select_action(state)

                # Ejecutar acción
                next_state, reward, done, info = self.env.step(action)

                # Almacenar recompensa
                self.agent.store_reward(reward, done)

                episode_reward += reward
                state = next_state

                if done:
                    if info['cup_flipped']:
                        success_count += 1
                    break

            episode_rewards.append(episode_reward)

            # Actualizar política
            if episode % update_interval == 0:
                self.agent.update()

            # Estadísticas y logging
            if episode % print_interval == 0:
                avg_reward = np.mean(episode_rewards[-print_interval:])
                success_rate = (success_count / episode) * 100

                print(f"Episodio {episode:4d} | "
                      f"Recompensa promedio: {avg_reward:7.2f} | "
                      f"Tasa de éxito: {success_rate:5.1f}%")

                # Guardar estadísticas
                self.training_stats['episodes'].append(episode)
                self.training_stats['rewards'].append(avg_reward)
                self.training_stats['success_rate'].append(success_rate)
                self.training_stats['average_steps'].append(
                    np.mean([info['step'] for _ in range(print_interval)])
                )

        print("\n" + "="*50)
        print("ENTRENAMIENTO COMPLETADO")
        print(f"Tasa de éxito final: {(success_count / max_episodes) * 100:.1f}%")
        print(f"Recompensa promedio final: {np.mean(episode_rewards[-100:]):.2f}")
        print("="*50)

    def evaluate(self, num_episodes: int = 10, render: bool = True):
        """Evalúa el agente entrenado"""
        print("\nEvaluando agente entrenado...")

        success_count = 0
        total_rewards = []

        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0

            for step in range(self.env.max_steps):
                action = self.agent.select_action(state)
                state, reward, done, info = self.env.step(action)
                episode_reward += reward

                if render and episode < 3:  # Mostrar solo primeros 3 episodios
                    self.env.render()

                if done:
                    if info['cup_flipped']:
                        success_count += 1
                        print(f"Episodio {episode + 1}: ¡ÉXITO! Recompensa: {episode_reward:.2f}")
                    else:
                        print(f"Episodio {episode + 1}: Falló. Recompensa: {episode_reward:.2f}")
                    total_rewards.append(episode_reward)
                    break

        print(f"\nResultados de evaluación:")
        print(f"Tasa de éxito: {(success_count / num_episodes) * 100:.1f}%")
        print(f"Recompensa promedio: {np.mean(total_rewards):.2f}")
        print(f"Desviación estándar: {np.std(total_rewards):.2f}")

    def plot_training_stats(self):
        """Grafica las estadísticas de entrenamiento"""
        if not self.training_stats['episodes']:
            print("No hay estadísticas de entrenamiento disponibles.")
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # Recompensas
        ax1.plot(self.training_stats['episodes'], self.training_stats['rewards'])
        ax1.set_title('Recompensa Promedio por Episodio')
        ax1.set_xlabel('Episodio')
        ax1.set_ylabel('Recompensa')
        ax1.grid(True)

        # Tasa de éxito
        ax2.plot(self.training_stats['episodes'], self.training_stats['success_rate'])
        ax2.set_title('Tasa de Éxito')
        ax2.set_xlabel('Episodio')
        ax2.set_ylabel('Éxito (%)')
        ax2.grid(True)

        # Pasos promedio
        if self.training_stats['average_steps']:
            ax3.plot(self.training_stats['episodes'], self.training_stats['average_steps'])
            ax3.set_title('Pasos Promedio por Episodio')
            ax3.set_xlabel('Episodio')
            ax3.set_ylabel('Pasos')
            ax3.grid(True)

        # Histograma de recompensas
        ax4.hist(self.training_stats['rewards'], bins=20, alpha=0.7)
        ax4.set_title('Distribución de Recompensas')
        ax4.set_xlabel('Recompensa')
        ax4.set_ylabel('Frecuencia')
        ax4.grid(True)

        plt.tight_layout()
        plt.pause(0.01)
        plt.clf()


def main():
    """Función principal para ejecutar el entrenamiento"""
    print("🤖 Sistema de Aprendizaje por Refuerzo para Brazo Robótico")
    print("=" * 60)

    # Crear entorno
    env = RoboticArmEnvironment(workspace_size=1.0, max_steps=200)

    # Obtener dimensiones del espacio de estados y acciones
    sample_state = env.reset()
    state_dim = len(sample_state)
    action_dim = env.num_joints + 1  # Articulaciones + acción de empuje

    print(f"Dimensión del espacio de estados: {state_dim}")
    print(f"Dimensión del espacio de acciones: {action_dim}")
    print(f"Número de articulaciones: {env.num_joints}")
    print(f"Área de trabajo: {env.workspace_size}m")

    # Crear agente PPO
    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        lr=3e-4,
        eps_clip=0.2,
        k_epochs=4,
        gamma=0.99
    )

    # Crear gestor de entrenamiento
    trainer = TrainingManager(env, agent)

    # Entrenar agente
    trainer.train(
        max_episodes=1000,
        update_interval=20,
        print_interval=50
    )

    # Guardar modelo
    agent.save_model("robotic_arm_ppo_model.pth")
    print("\nModelo guardado como 'robotic_arm_ppo_model.pth'")

    # Evaluar agente
    trainer.evaluate(num_episodes=10, render=False)

    # Mostrar estadísticas
    trainer.plot_training_stats()

    print("\n🎉 ¡Entrenamiento completado exitosamente!")
    return trainer, agent, env


if __name__ == "__main__":
    trainer, agent, env = main()